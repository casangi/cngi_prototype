{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YnE3V8l-vy5"
   },
   "source": [
    "# Benchmarks\n",
    "\n",
    "A demonstration of parallel processing performance of this CNGI prototype against the current released versions of CASA using a selection of ALMA datasets representing different computationally-demanding configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7bP3XvlAmSh"
   },
   "source": [
    "## Methodology\n",
    "\n",
    "Measurement of runtime performance for the component typically dominating compute cost for existing and future workflows -- the [aperture synthesis gridder](https://casadocs.readthedocs.io/en/latest/notebooks/synthesis_imaging.html#Gridding-+-FFT) -- was made against the reference implementation of CASA. Both the latest public release ([6.1.2-7](https://casa.nrao.edu/casadocs/casa-6.1.0/introduction/release-notes-610), referenced here as 6.1) and a pre-release build of the next upcoming CASA version (referenced here as 6.2) were used in order to demonstrate the effect of the recent major refactor to the gridding code of cube imaging implemented by the production system.\n",
    "\n",
    "Relevant calls of the CASA task `tclean` were isolated for direct comparison with the latest version of the `cngi-prototype` implementation of the mosaic and standard gridders.\n",
    "\n",
    "The steps of the workflow used to prepare data for testing were:\n",
    "\n",
    "1. Download archive data from ALMA Archive\n",
    "2. Restore calibrated MeasurementSet using scriptForPI.py with compatible version of CASA\n",
    "3. Split off science targets and representative spectral window into a single MeasurementSet\n",
    "4. Convert calibrated MeasurementSet into zarr format using `cngi.conversion.convert_ms`\n",
    "\n",
    "This allowed for generation of image data from visibilities for comparison. Tests were run in two different environments:\n",
    "\n",
    "1. On premises using the same high performance computing (HPC) cluster environment used for offline processing of data from North American [ALMA](https://science.nrao.edu/facilities/alma/facilities/alma) operations.\n",
    "2. Using commercial cloud resources furnished by Amazon Web Services ([AWS](https://aws.amazon.com/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlK-C6cQAgG3"
   },
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Observations were chosen for their source/observation properties, data volume, and usage mode diversity, particularly the relatively large number of spectral channels, pointings, or executions. Two were observed by the ALMA Compact Array (ACA) of 7m antennas, and two were observed by the main array of 12m antennas.\n",
    "\n",
    "The datasets from each project code and Member Object Unit Set (MOUS) were processed following publicly documented ALMA archival reprocessing workflows, and come from public observations used used by other teams in previous benchmarking and profiling efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017.1.00271.S \n",
    "\n",
    "Compact array observations over many (nine) execution blocks using the mosaic gridder \n",
    "\n",
    "-   MOUS uid://A001/X1273/X2e3\n",
    "-   MeasurementSet Rows: 315831\n",
    "-   CNGI Shape (time, baseline, chan, pol): (455, 745, 7635, 2)\n",
    "-   Visibility Data Size: 82.82 GB\n",
    "\n",
    "\n",
    "![im10](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/X2e3/combined_X2e3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvtnCh7pAoOI"
   },
   "source": [
    "### 2018.1.01091.S\n",
    "\n",
    "Compact array observations with many (141) pointings using the mosaic gridder\n",
    "\n",
    "-   MOUS uid://A001/X133d/X1a36\n",
    "-   MeasurementSet Rows: 15510\n",
    "-   CNGI Shape (time, baseline, chan, pol): (282, 55, 1024, 2)\n",
    "-   Visibility Data Size: 508.23 MB\n",
    "\n",
    "![im2](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/X1a36/combined_X1a36.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4jN2RRAgZf"
   },
   "source": [
    "### 2017.1.00717.S\n",
    "\n",
    "Main array observations with many spectral channels and visibilities using the standard gridder\n",
    "\n",
    "-   MOUS uid://A001/X1273/Xc66\n",
    "-   MeasurementSet Rows: 100284\n",
    "-   CNGI Shape (time, baseline, chan, pol): (2564, 53, 2048, 2)\n",
    "-   Visibility Data Size: 8.91 GB\n",
    "\n",
    "![im6](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/Xc66/combined_Xc66.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIXchZ23Kdzk"
   },
   "source": [
    "### 2017.1.00983.S \n",
    "\n",
    "Main array observations with many spectral channels and visibilities using the mosaic gridder\n",
    "\n",
    "-   MOUS uid://A001/X12a3/X3be\n",
    "-   MeasurementSet Rows: 646418\n",
    "-   CNGI Shape (time, baseline, chan, pol): (729, 1159, 3853, 2)\n",
    "-   Visibility Data Size: 104.17 GB\n",
    "\n",
    "![im14](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/Xc66/combined_Xc66.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed Up and Work**\n",
    "\n",
    "![im340](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/gcf_size_cluster_A001_X12a3_X3be.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzXolTYzE7LM"
   },
   "source": [
    "## Comparison of Runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6JL38cDV_wX"
   },
   "source": [
    "**Single Machine**\n",
    "\n",
    "The total runtime of the prototype mosaic gridder was less than the 6.1 and 6.2 reference implementations in most cases. The prototype standard gridder has comparable performance for all but the least-optimal chunk size selection. \n",
    "\n",
    "There does not appear to be a performance penalty associated with the adoption of a pure Python framework in comparison to the compiled C++/Fortran reference implementation. This is likely due in large part to the prototype's reliance on the `numba` Just-In-Time (JIT) transpiler and the C foreign function interface relied on by third-party framework packages including `numpy` and `scipy`.\n",
    "\n",
    "The Fortran gridding code in CASA appears slightly more efficient than the JIT-decorated Python code in the prototype. However,Â the test implementation more efficiently handles chunked data and does not have intermediate steps where data is written to disk, whereas CASA generates TempLattice files to store intermediate files.\n",
    "\n",
    "**Multi-Node**\n",
    "\n",
    "The total runtime of the prototype mosaic and standard gridders was less than the 6.1 and 6.2 reference implementations in all cases. \n",
    "\n",
    "There does not appear to be a performance penalty associated with the adoption of a pure Python framework for distributed scheduling in comparison to the MPI-based reference implementation. This is likely due in part to the graph optimization of the task scheduler, which includes overhead that begins to dominate the total runtime at higher levels of concurrency.\n",
    "\n",
    "\n",
    "**Comparison of CASA versions**\n",
    "\n",
    "Only the total time is comparable between test executions of CASA versions before and after cube refactor due to the difference of virtual concatenation vs. disk write using temp lattices. \n",
    "\n",
    "Note that for some settings of dask array chunking, one dask chunk had a shape smaller than the others due to combination of multiple executions before conversion. This effectively separated on-disk chunk shape along time time dimension, with some limited potential to degrade performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHILES Benchmark**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApYTqn0AXm6K"
   },
   "source": [
    "## Commercial Cloud\n",
    "\n",
    "The total runtime curves for tests run on AWS show higher variance. One contributing factor that likely dominated this effect was the use of [preemptible instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html) underlying the compute nodes running the worker configuration. For this same reason, some cloud-based test runs show decreased performance with increased scale. This is due to the preemption of nodes and associated redeployment by kubernetes, which sometimes constituted a large fraction of the total test runtime, as demonstrated by the task stream for the following test case. Note the horizontal white bar (signifying to tasks executed) shortly after graph execution begins, as well as some final tasks being assigned to a new node that came online after a few minutes (represented by the new \"bar\" of 8 rows at top right) in the following figure:\n",
    "\n",
    "![im18](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/task_stream_A001_X1273_Xc66_threads_40_chans_45.png)\n",
    "\n",
    "Qualitatively, failure rates were higher during tests of CASA on local HPC infrastructure than they were using dask on the cluster or cloud. The cube refactor shows a noticeable improvement in this area, but still worse than the prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPReEyNSj4SQ"
   },
   "source": [
    "## Profiling Results\n",
    "\n",
    "Benchmarks performed using a single chunk size constitute a test of strong scaling (constant data volume, changing number of processors). Three of the projects used the mosaic gridder, the test of which consisted of multiple function calls composed into a rudimentary \"pipeline\". The other project used the standard gridder, with fewer separate function calls and thus relatively more time spent on compute \n",
    "\n",
    "The time spent in each function `<plots>`\n",
    "\n",
    "The communication of data between workers constituted a relatively small proportion of the total runtime, and the distribution of data between workers was relatively uniform, at all horizontal scalings, with some hot spots beginning to present once tens of nodes were involved. This is demonstrated by the following figure, taken from the performance report of a representative test execution:\n",
    "![im17](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/bandwidth_A001_X12a3_X3be_threads_256_chans_48.png)\n",
    "\n",
    "The time overhead associated with graph creation and task scheduling (approximately 100 ms per task for dask) grew as more nodes were introduced until eventually coming to represent a fraction of total execution time comparable to the computation itself, especially in the test cases with smaller data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IzribuKmOJD"
   },
   "source": [
    "## Reference Configurations\n",
    "\n",
    "Dask profiling data were collected using the [`performance_report`](https://distributed.dask.org/en/latest/diagnosing-performance.html#performance-reports) function in tests run both on-premises and in the commercial cloud.\n",
    "\n",
    "Some values of the [distributed configuration](https://distributed.dask.org/en/latest/worker.html) were modified from their defaults:\n",
    "```\n",
    "distributed:\n",
    "  worker:\n",
    "    # Fractions of worker memory at which we take action to avoid memory blowup\n",
    "    # Set any of the lower three values to False to turn off the behavior entirely\n",
    "    memory:\n",
    "      target: 0.85  # fraction to stay below (default 0.60)\n",
    "      spill: 0.92  # fraction at which we spill to disk (default 0.70)\n",
    "      pause: 0.95  # fraction at which we pause worker threads (default 0.80)\n",
    "```\n",
    "\n",
    "Thread based parallelism in dependent libraries was disabled using environment variables `BLAS_NUM_THREADS`, `BLOSC_NOLOCK`, `MKL_NUM_THREADS`, and `OMP_NUM_THREADS`\n",
    "\n",
    "**On-premises HPC cluster**\n",
    "\n",
    "- Test execution via Python scripts submitted to Moab scheduler and Torque resource manager with specifications documented [internally](https://info.nrao.edu/computing/guide/cluster-processing)\n",
    "- Scheduling backend: `dask-jobqueue`\n",
    "- I/O of visibility and image data via shared infiniband-interconnected lustre file system for access from on-premises high performance compute (HPC) nodes\n",
    "- 16 threads per dask worker\n",
    "- Compute via nodes from the cvpost batch queue with Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz with clock speed 1199.865 MHz and cache size 20480 KB.\n",
    "\n",
    "**Commercial cloud (AWS)**\n",
    "\n",
    "- Test execution via Jupyter notebooks running on a cloud deployment of the public [dask-docker](https://docs.dask.org/en/latest/setup/docker.html) image (version 2021.3.0) backed by a [Kubernetes cluster](https://docs.dask.org/en/latest/setup/kubernetes-helm.html) installed with `kops` (version 1.18.0), modified to include installation of version 0.0.83 of `cngi-prototype` and associated dependencies.\n",
    "- Distributed scheduling backend: `dask.distributed`\n",
    "- I/O of visibility and image data via Simple Storage Service (S3) object storage for access from commercial cloud Elastic Compute Cloud (EC2) nodes\n",
    "- 8 threads per dask worker\n",
    "- Compute via managed Kubernetes cluster backed by a variety of [instance types](https://aws.amazon.com/ec2/instance-types/) all running on the current daily build of the [Ubuntu 20.04](http://cloud-images.ubuntu.com/focal/current/) operating system. Cluster coordination service pods were run on a single dedicated `t3.small` instance. Jupyter notebook, dask scheduler, and [etcd](https://etcd.io/) service pods were run on a single dedicated `m5dn.4xlarge` instance. Worker pods were run on a configured number of preemptible instances drawn from a pool composed of the following types: `m5.4xlarge`, `m5d.4xlarge`, `m5dn.4xlarge`, `r5.4xlarge`, `r4.4xlarge`,`m4.4xlarge`.\n",
    "\n",
    "Hyperthreads [exposed as vCPUs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-optimize-cpu.html) on the EC2 instances were disabled using the following shell script at instance launch:\n",
    "```\n",
    "spec:\n",
    "  additionalUserData:\n",
    "  - content: |\n",
    "      #!/usr/bin/env bash\n",
    "      for cpunum in $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\\n' | sort -un)\n",
    "      do\n",
    "        echo 0 > /sys/devices/system/cpu/cpu$cpunum/online\n",
    "      done\n",
    "    name: disable_hyperthreading.sh\n",
    "    type: text/x-shellscript\n",
    "  image: \n",
    "```\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "benchmarking.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
