{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of benchmarking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casangi/cngi_prototype/blob/master/benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnE3V8l-vy5"
      },
      "source": [
        "# Benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "As CASA next-gen development proceeds, applications designed to measure a suite of agreed-upon benchmarks may eventually prove useful or necessary. For this proof of concept, performance benchmarking and profiling were carried out on an ad hoc basis throughout development and then, after the functionality under demonstration stabilized, more formally against a set of data taken to be generally representative. This notebook contains no executable code cells, but serves to illustrate the process and results following the most recent round of performance testing.\n",
        "\n",
        "- Description of test methodology and datasets\n",
        "- Per-dataset test results\n",
        "- Discussion and analysis of results\n",
        "- Appendix for reference/publication of parquet files with timing data?\n",
        "- Earlier single-machine and cloud tests, [memory profiling](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/bench_top.png) **if we decide to include this**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7bP3XvlAmSh"
      },
      "source": [
        "## Methodology\n",
        "\n",
        "Measurement of runtime performance for the component typically dominating compute cost for existing and future workflows -- the [aperture synthesis gridder](https://casadocs.readthedocs.io/en/latest/notebooks/synthesis_imaging.html#Gridding-+-FFT) -- was made against the reference implementation of CASA. Both the latest public release ([6.1.2-7](https://casa.nrao.edu/casadocs/casa-6.1.0/introduction/release-notes-610), referenced here as 6.1) and a static branch from the development trunk ([CAS-13399](https://open-jira.nrao.edu/browse/CAS-13399), referenced here as 6.2) were used in order to demonstrate the effect of the recent major refactor to the gridding code of cube imaging implemented by the production system.\n",
        "\n",
        "Relevant calls of the CASA task `tclean` were isolated for direct comparison with the latest version of the `cngi-prototype` implementation of the mosaic and standard gridders.\n",
        "\n",
        "The steps of the workflow used to prepare data for testing were:\n",
        "\n",
        "1. Download archive data from ALMA Archive\n",
        "2. Restore calibrated MeasurementSet using scriptForPI.py with compatible version of CASA\n",
        "3. Split off science targets and representative spectral window into a single MeasurementSet\n",
        "4. Convert calibrated MeasurementSet into zarr format using `cngi.conversion.convert_ms`\n",
        "\n",
        "This allowed for generation of image data from visibilities for comparison. Tests were run in two different environments:\n",
        "\n",
        "1. On premises using the same high performance computing (HPC) cluster environment used for offline processing of data from North American [ALMA](https://science.nrao.edu/facilities/alma/facilities/alma) operations.\n",
        "2. Using commercial cloud resources furnished by Amazon Web Services ([AWS](https://aws.amazon.com/))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlK-C6cQAgG3"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Datasets from each project code and Member Object Unit Set (MOUS) were processed following publicly documented ALMA archival reprocessing workflows, and come from public observations used used by other teams in previous benchmarking and profiling efforts.\n",
        "\n",
        "Observations were chosen for their source/observation properties, data volume, and usage mode diversity, particularly the relatively large number of spectral channels, pointings, or executions. Two were observed by the ALMA Compact Array (ACA) of 7m antennas, and two were observed by the main array of 12m antennas.\n",
        "\n",
        "The datasets are presented here from smallest to largest total visibility volume, but generally exercise two different categories.\n",
        "\n",
        "Benchmarks using a dataset larger than total memory are underway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvtnCh7pAoOI"
      },
      "source": [
        "### 2018.1.01091.S\n",
        "\n",
        "Project title: Mapping M17: the best galactic laboratory for measuring the role of photoionizing feedback\n",
        "\n",
        "MOUS uid://A001/X133d/X1a36\n",
        "\n",
        "Compact array observations with many (141) pointings using the mosaic gridder\n",
        "\n",
        "MeasurementSet Rows: 15510\n",
        "\n",
        "Converted visibility data dimensions and uncompressed volume of the DATA array (and chunks, for a given factor) are shown below:\n",
        "\n",
        "![im1](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/data_repr_A001_X133d_X1a36_chans_10.png)\n",
        "\n",
        "![im2](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/node_A001_X133d_X1a36.png)\n",
        "\n",
        "![im3](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/cluster_A001_X133d_X1a36.png)\n",
        "\n",
        "![im4](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/aws_A001_X133d_X1a36.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4jN2RRAgZf"
      },
      "source": [
        "### 2017.1.00271.S \n",
        "\n",
        "Project title: Why is ~ 1/4 of the LMC's molecular gas not forming massive stars?\n",
        "\n",
        "MOUS uid://A001/X1273/Xc66\n",
        "\n",
        "Compact array observations over many (nine) execution blocks using the mosaic gridder \n",
        "\n",
        "MeasurementSet Rows: 100284\n",
        "\n",
        "Converted visibility data dimensions and uncompressed volume of the DATA array (and chunks, for a given factor) are shown below:\n",
        "\n",
        "![im5](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/data_repr_A001_X1273_Xc66_chans_16.png)\n",
        "\n",
        "![im6](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/node_A001_X1273_Xc66.png)\n",
        "\n",
        "![im7](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/cluster_A001_X1273_Xc66.png)\n",
        "\n",
        "![im8](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/aws_A001_X1273_Xc66.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJUbfLR3KT-4"
      },
      "source": [
        "### 2017.1.00717.S\n",
        "\n",
        "Project title: Astrochemical ABCs - An ALMA Band 9/10 Chemical Survey of NGC 6334I\n",
        "\n",
        "MOUS uid://A001/X1273/X2e3\n",
        "\n",
        "Main array observations with many spectral channels and visibilities using the standard gridder\n",
        "\n",
        "MeasurementSet Rows: 315831\n",
        "\n",
        "Converted visibility data dimensions and uncompressed volume of the DATA array (and chunks, for a given factor) are shown below:\n",
        "\n",
        "![im9](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/data_repr_A001_X1273_X2e3_chans_45.png)\n",
        "\n",
        "![im10](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/node_A001_X1273_X2e3.png)\n",
        "\n",
        "![im11](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/cluster_A001_X1273_X2e3.png)\n",
        "\n",
        "![im12](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/aws_A001_X1273_X2e3.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIXchZ23Kdzk"
      },
      "source": [
        "### 2017.1.00983.S \n",
        "\n",
        "Project title: Quantifying the Feedback Potential of Young Massive Protoclusters\n",
        "\n",
        "MOUS uid://A001/X12a3/X3be\n",
        "\n",
        "Main array observations with many spectral channels and visibilities using the mosaic gridder\n",
        "\n",
        "MeasurementSet Rows: 646418\n",
        "\n",
        "Converted visibility data dimensions and uncompressed volume of the DATA array (and chunks, for a given factor) are shown below:\n",
        "\n",
        "![im13](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/data_repr_A001_X12a3_X3be_chans_48.png)\n",
        "\n",
        "![im14](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/node_A001_X12a3_X3be.png)\n",
        "\n",
        "![im15](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/cluster_A001_X12a3_X3be.png)\n",
        "\n",
        "![im16](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/aws_A001_X12a3_X3be.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXolTYzE7LM"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6JL38cDV_wX"
      },
      "source": [
        "### Comparison of single machine runtime\n",
        "\n",
        "The total runtime of the prototype mosaic gridder wass less than the 6.1 and 6.2 reference implementations in most cases. The prototype standard gridder has comparable performance for all but the least-optimal chunk size selection. \n",
        "\n",
        "There does not appear to be a performance penalty associated with the adoption of a pure Python framework in comparison to the compiled C++/Fortran reference implementation. This is likely due in large part to the prototype's reliance on the `numba` Just-In-Time (JIT) transpiler and the C foreign function interface relied on by third-party framework packages including `numpy` and `scipy`.\n",
        "\n",
        "The Fortran gridding code in CASA appears slightly more efficient than the JIT-decorated Python code in the prototype. However,Â the test implementation more efficiently handles chunked data and does not have intermediate steps where data is written to disk, whereas CASA generates TempLattice files to store intermediate files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dEknPPSXY0J"
      },
      "source": [
        "### Comparison of multi-node runtime\n",
        "\n",
        "The total runtime of the prototype mosaic and standard gridders was less than the 6.1 and 6.2 reference implementations in all cases. \n",
        "\n",
        "There does not appear to be a performance penalty associated with the adoption of a pure Python framework for distributed scheduling in comparison to the MPI-based reference implementation. This is likely due in part to the graph optimization of the task scheduler, which includes overhead that begins to dominate the total runtime at higher levels of concurrency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8w2w475NBi1"
      },
      "source": [
        "### Comparison of CASA versions\n",
        "\n",
        "Only the total time is comparable between test executions of CASA versions before and after cube refactor due to the difference of virtual concatenation vs. disk write using temp lattices. \n",
        "\n",
        "Note that for some settings of dask array chunking, one dask chunk had a shape smaller than the others due to combination of multiple executions before conversion. This effectively separated on-disk chunk shape along time time dimension, with some limited potential to degrade performance.\n",
        "\n",
        "**Ganglia plots on nodes during latest single node test runs?** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPReEyNSj4SQ"
      },
      "source": [
        "### Comparison of profiling results\n",
        "\n",
        "Benchmarks performed using a single chunk size constitute a test of strong scaling (constant data volume, changing number of processors). Three of the projects used the mosaic gridder, the test of which consisted of multiple function calls composed into a rudimentary \"pipeline\". The other project used the standard gridder, with fewer separate function calls and thus relatively more time spent on compute \n",
        "\n",
        "The time spent in each function `<plots>`\n",
        "\n",
        "The communication of data between workers constituted a relatively small proportion of the total runtime, and the distribution of data between workers was relatively uniform, at all horizontal scalings, with some hot spots beginning to present once tens of nodes were involved. This is demonstrated by the following figure, taken from the performance report of a representative test execution:\n",
        "![im17](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/bandwidth_A001_X12a3_X3be_threads_256_chans_48.png)\n",
        "\n",
        "The time overhead associated with graph creation and task scheduling (approximately 100 ms per task for dask) grew as more nodes were introduced until eventually coming to represent a fraction of total execution time comparable to the computation itself, especially in the test cases with smaller data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApYTqn0AXm6K"
      },
      "source": [
        "### Comparison of on-premises and commercial cloud runtime\n",
        "\n",
        "The total runtime curves for tests run on AWS show higher variance. One contributing factor that likely dominated this effect was the use of [preemptible instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html) underlying the compute nodes running the worker configuration. For this same reason, some cloud-based test runs show decreased performance with increased scale. This is due to the preemption of nodes and associated redeployment by kubernetes, which sometimes constituted a large fraction of the total test runtime, as demonstrated by the task stream for the following test case. Note the horizontal white bar (signifying to tasks executed) shortly after graph execution begins, as well as some final tasks being assigned to a new node that came online after a few minutes (represented by the new \"bar\" of 8 rows at top right) in the following figure:\n",
        "\n",
        "![im18](https://raw.githubusercontent.com/casangi/cngi_prototype/master/docs/_media/task_stream_A001_X1273_Xc66_threads_40_chans_45.png)\n",
        "\n",
        "Qualitatively, failure rates were higher during tests of CASA on local HPC infrastructure than they were using dask on the cluster or cloud. The cube refactor shows a noticeable improvement in this area, but still worse than the prototype."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IzribuKmOJD"
      },
      "source": [
        "### Configuration of computing resources\n",
        "\n",
        "Dask profiling data were collected using the [`performance_report`](https://distributed.dask.org/en/latest/diagnosing-performance.html#performance-reports) function in tests run both on-premises and in the commercial cloud.\n",
        "\n",
        "Some values of the [distributed configuration](https://distributed.dask.org/en/latest/worker.html) were modified from their defaults:\n",
        "```\n",
        "distributed:\n",
        "  worker:\n",
        "    # Fractions of worker memory at which we take action to avoid memory blowup\n",
        "    # Set any of the lower three values to False to turn off the behavior entirely\n",
        "    memory:\n",
        "      target: 0.85  # fraction to stay below (default 0.60)\n",
        "      spill: 0.92  # fraction at which we spill to disk (default 0.70)\n",
        "      pause: 0.95  # fraction at which we pause worker threads (default 0.80)\n",
        "```\n",
        "\n",
        "Thread based parallelism in dependent libraries was disabled using environment variables `BLAS_NUM_THREADS`, `BLOSC_NOLOCK`, `MKL_NUM_THREADS`, and `OMP_NUM_THREADS`\n",
        "\n",
        "#### On-premises HPC cluster\n",
        "\n",
        "- Test execution via Python scripts submitted to Moab scheduler and Torque resource manager with specifications documented [internally](https://info.nrao.edu/computing/guide/cluster-processing)\n",
        "- Scheduling backend: `dask-jobqueue`\n",
        "- I/O of visibility and image data via shared infiniband-interconnected lustre file system for access from on-premises high performance compute (HPC) nodes\n",
        "- 16 threads per dask worker\n",
        "- Compute via nodes from the cvpost batch queue with Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz with clock speed 1199.865 MHz and cache size 20480 KB.\n",
        "\n",
        "#### Commercial cloud (AWS)\n",
        "\n",
        "- Test execution via Jupyter notebooks running on a cloud deployment of the public [dask-docker](https://docs.dask.org/en/latest/setup/docker.html) image (version 2021.3.0) backed by a [Kubernetes cluster](https://docs.dask.org/en/latest/setup/kubernetes-helm.html) installed with `kops` (version 1.18.0), modified to include installation of version 0.0.83 of `cngi-prototype` and associated dependencies.\n",
        "- Distributed scheduling backend: `dask.distributed`\n",
        "- I/O of visibility and image data via Simple Storage Service (S3) object storage for access from commercial cloud Elastic Compute Cloud (EC2) nodes\n",
        "- 8 threads per dask worker\n",
        "- Compute via managed Kubernetes cluster backed by a variety of [instance types](https://aws.amazon.com/ec2/instance-types/) all running on the current daily build of the [Ubuntu 20.04](http://cloud-images.ubuntu.com/focal/current/) operating system. Cluster coordination service pods were run on a single dedicated `t3.small` instance. Jupyter notebook, dask scheduler, and [etcd](https://etcd.io/) service pods were run on a single dedicated `m5dn.4xlarge` instance. Worker pods were run on a configured number of preemptible instances drawn from a pool composed of the following types: `m5.4xlarge`, `m5d.4xlarge`, `m5dn.4xlarge`, `r5.4xlarge`, `r4.4xlarge`,`m4.4xlarge`.\n",
        "\n",
        "**Furgher details possibly better communicated as an update to Installation section of readthedocs**\n",
        "\n",
        "Hyperthreads [exposed as vCPUs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-optimize-cpu.html) on the EC2 instances were disabled using the following shell script at instance launch:\n",
        "```\n",
        "spec:\n",
        "  additionalUserData:\n",
        "  - content: |\n",
        "      #!/usr/bin/env bash\n",
        "      for cpunum in $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\\n' | sort -un)\n",
        "      do\n",
        "        echo 0 > /sys/devices/system/cpu/cpu$cpunum/online\n",
        "      done\n",
        "    name: disable_hyperthreading.sh\n",
        "    type: text/x-shellscript\n",
        "  image: \n",
        "```\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZsiR8AoN6Sl"
      },
      "source": [
        "### Comparison with similar projects\n",
        "\n",
        "#### Pangeo\n",
        "\n",
        "Other open source projects that rely on similar framework components have produced their own benchmarking efforts, which can serve as a useful reference. One example is [Pangeo](https://github.com/pangeo-data/benchmarking), who as of July 2020 have managed deployments on [multiple HPC environments](https://pangeo.io/deployments.html#high-performance-computing-deployments) with processing capability on the order of 100+ TeraFLOP/S.\n",
        "\n",
        "#### SARAO\n",
        "\n",
        "Developers from the South African Radio Astronomy Observatory (SARAO) are performing experiments using a framework almost identical to that adopted by our prototype. This work has already led to the development of various prototypes with direct application to the same problem space. Some publication of results (see [here](https://schedule.adass2020.es/adass2020/talk/KPMRYS/) and [here](https://schedule.adass2020.es/adass2020/talk/CCABCE/)) and analysis of performance at scale is underway, and will be a target of collaborative investigation in the future.\n",
        "\n",
        "#### RCI\n",
        "\n",
        "The Radio Camera Initiative (RCI) is a project under development to create a near-real-time flagging, calibration and imaging analysis pipeline, bridging the traditional domains of \"online\" and \"offline\" data processing for astronomical radio interferometry. The RCI has planned for an [imaging contest](https://www.radiocamera.io/image-contest) (after the style of [Kaggle](https://www.kaggle.com/) data science competitions) to serve as a community benchmark of algorithms and implementations in use by our field.\n"
      ]
    }
  ]
}